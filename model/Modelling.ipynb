{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 15:54:01.107158  9796 file_utils.py:39] PyTorch version 1.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import string\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from model import SentenceBert\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 15:54:04.062693  9796 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0708 15:54:04.066643  9796 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 15:54:04.068637  9796 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0708 15:54:07.584614  9796 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0708 15:54:07.585603  9796 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0708 15:54:07.585603  9796 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0708 15:54:07.586573  9796 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0708 15:54:08.505114  9796 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0708 15:54:08.507106  9796 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 15:54:08.575949  9796 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9638e320d67444cc9d3d291d194758d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=38.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>forex</th>\n",
       "      <th>finance</th>\n",
       "      <th>stocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beware of debt costs and an inflationary bite,...</td>\n",
       "      <td>2020-07-05 19:17:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>0.498809</td>\n",
       "      <td>0.287072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deals of the day-Mergers and acquisitions</td>\n",
       "      <td>2020-07-07 16:00:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.157465</td>\n",
       "      <td>0.358413</td>\n",
       "      <td>0.328640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zoom rolls out hardware subscription service</td>\n",
       "      <td>2020-07-07 10:48:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.101178</td>\n",
       "      <td>0.283011</td>\n",
       "      <td>0.171321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In Hong Kong national security law, echoes of ...</td>\n",
       "      <td>2020-07-07 08:47:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.074657</td>\n",
       "      <td>0.224370</td>\n",
       "      <td>0.027718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China converts Hong Kong hotel into new nation...</td>\n",
       "      <td>2020-07-08 01:12:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.122923</td>\n",
       "      <td>0.223557</td>\n",
       "      <td>0.051019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Beware of debt costs and an inflationary bite,...   \n",
       "1          Deals of the day-Mergers and acquisitions   \n",
       "2       Zoom rolls out hardware subscription service   \n",
       "3  In Hong Kong national security law, echoes of ...   \n",
       "4  China converts Hong Kong hotel into new nation...   \n",
       "\n",
       "                       date   query     forex   finance    stocks  \n",
       "0 2020-07-05 19:17:00-08:00  google  0.064136  0.498809  0.287072  \n",
       "1 2020-07-07 16:00:00-08:00  google  0.157465  0.358413  0.328640  \n",
       "2 2020-07-07 10:48:00-08:00  google  0.101178  0.283011  0.171321  \n",
       "3 2020-07-07 08:47:00-08:00  google  0.074657  0.224370  0.027718  \n",
       "4 2020-07-08 01:12:00-08:00  google  0.122923  0.223557  0.051019  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = joblib.load(\"../data/reuters_news.joblib\")\n",
    "df = df.drop_duplicates(subset=\"title\")\n",
    "labels = ['forex', 'finance', 'stocks']\n",
    "SB = SentenceBert()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    sim_dict = SB.get_similarity(row[\"title\"], labels)\n",
    "    for i in range(len(labels)):   \n",
    "        df.loc[index, labels[i]] = sim_dict[labels[i]]\n",
    "df = df.sort_values(by=\"finance\", axis=0, ascending=False)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 15:54:16.824857  9796 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0708 15:54:16.828886  9796 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 15:54:17.702512  9796 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tokenizer.encode_plus(\n",
    "    df[\"title\"][0], \n",
    "    max_length=32, \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    pad_to_max_length=True, \n",
    "    return_attention_mask=True, \n",
    "    return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 15:55:32.213019  9796 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0708 15:55:32.216011  9796 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 15:55:32.345652  9796 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built!\n"
     ]
    }
   ],
   "source": [
    "class ReutersClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, p=0.25):\n",
    "        super(ReutersClassifier, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.classifier = nn.Linear(self.bert_layer.config.dim, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert_layer(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask)\n",
    "        main = self.dropout(pooled_output[0][:, 0, :])\n",
    "        return F.sigmoid(self.classifier(main))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = ReutersClassifier(n_classes=1)\n",
    "model.to(device)\n",
    "print(\"Model built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilty = model(sample[\"input_ids\"].to(device), sample[\"attention_mask\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6039]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wj",
   "language": "python",
   "name": "wj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
