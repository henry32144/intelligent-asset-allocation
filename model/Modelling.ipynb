{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import string\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = joblib.load(\"../data/train_top25_v3.bin\")\n",
    "valid = joblib.load(\"../data/valid_top25_v3.bin\")\n",
    "test = joblib.load(\"../data/test_top25_v3.bin\")\n",
    "\n",
    "train = train[train['content'].map(len) > 0]\n",
    "valid = valid[valid['content'].map(len) > 0]\n",
    "test = test[test['content'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>news_count</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>ticker</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-11</th>\n",
       "      <td>[Jan 4 (Reuters) - Apple Inc has appointed Ado...</td>\n",
       "      <td>1</td>\n",
       "      <td>29.16</td>\n",
       "      <td>29.39</td>\n",
       "      <td>28.96</td>\n",
       "      <td>29.32</td>\n",
       "      <td>3306300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-20</th>\n",
       "      <td>[SAN FRANCISCO (Reuters) - In the summer of 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.28</td>\n",
       "      <td>30.55</td>\n",
       "      <td>30.18</td>\n",
       "      <td>30.50</td>\n",
       "      <td>4091200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-23</th>\n",
       "      <td>[SAN FRANCISCO (Reuters) - In the summer of 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.33</td>\n",
       "      <td>30.78</td>\n",
       "      <td>29.98</td>\n",
       "      <td>30.23</td>\n",
       "      <td>5378200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-24</th>\n",
       "      <td>[SAN FRANCISCO (Reuters) - In the summer of 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.17</td>\n",
       "      <td>31.00</td>\n",
       "      <td>30.08</td>\n",
       "      <td>30.95</td>\n",
       "      <td>4715800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-25</th>\n",
       "      <td>[SAN FRANCISCO (Reuters) - In the summer of 20...</td>\n",
       "      <td>1</td>\n",
       "      <td>30.59</td>\n",
       "      <td>31.40</td>\n",
       "      <td>30.51</td>\n",
       "      <td>31.34</td>\n",
       "      <td>5459800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ADBE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      content  news_count  \\\n",
       "date                                                                        \n",
       "2012-01-11  [Jan 4 (Reuters) - Apple Inc has appointed Ado...           1   \n",
       "2012-01-20  [SAN FRANCISCO (Reuters) - In the summer of 20...           1   \n",
       "2012-01-23  [SAN FRANCISCO (Reuters) - In the summer of 20...           1   \n",
       "2012-01-24  [SAN FRANCISCO (Reuters) - In the summer of 20...           1   \n",
       "2012-01-25  [SAN FRANCISCO (Reuters) - In the summer of 20...           1   \n",
       "\n",
       "             Open   High    Low  Close   Volume  Dividends  Stock Splits  \\\n",
       "date                                                                       \n",
       "2012-01-11  29.16  29.39  28.96  29.32  3306300        0.0           0.0   \n",
       "2012-01-20  30.28  30.55  30.18  30.50  4091200        0.0           0.0   \n",
       "2012-01-23  30.33  30.78  29.98  30.23  5378200        0.0           0.0   \n",
       "2012-01-24  30.17  31.00  30.08  30.95  4715800        0.0           0.0   \n",
       "2012-01-25  30.59  31.40  30.51  31.34  5459800        0.0           0.0   \n",
       "\n",
       "           ticker  label  \n",
       "date                      \n",
       "2012-01-11   ADBE      1  \n",
       "2012-01-20   ADBE      1  \n",
       "2012-01-23   ADBE      0  \n",
       "2012-01-24   ADBE      1  \n",
       "2012-01-25   ADBE      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5553,  1018,  1006, 26665,  1007,  1011,  6207,  4297,  2038,\n",
       "          2805, 18106,  3001,  4297,  1521,  1055,  6927, 28774,  2229,  2072,\n",
       "          2000,  2132,  2049, 24264,  2094,  4684,  1011,  6475,  2449,  1010,\n",
       "         22950,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer.encode_plus(\n",
    "    train[\"content\"][0][0], \n",
    "    max_length=32, \n",
    "    add_special_tokens=True, \n",
    "    return_token_type_ids=False, \n",
    "    pad_to_max_length=True, \n",
    "    return_attention_mask=True, \n",
    "    return_tensors=\"pt\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[\"content\"][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.targets = df.label.values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        contents = self.df[\"content\"][item]\n",
    "        target = self.targets[item]\n",
    "\n",
    "        enc_list = []\n",
    "        for content in contents:\n",
    "            enc = self.tokenizer.encode_plus(\n",
    "                str(content),\n",
    "                max_length=self.max_len,\n",
    "                add_special_tokens=True,\n",
    "                return_token_type_ids=False,\n",
    "                pad_to_max_length=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\")\n",
    "            enc[\"input_ids\"] = enc[\"input_ids\"].flatten()\n",
    "            enc[\"attention_mask\"] = enc[\"attention_mask\"].flatten()\n",
    "            enc_list.append(enc)\n",
    "\n",
    "        return {\n",
    "            \"ids_and_mask\": enc_list,\n",
    "            \"target\": torch.tensor(target)\n",
    "        }\n",
    "\n",
    "\n",
    "def create_dataloader(df, tokenizer, max_len, batch_size, shuffle=True):\n",
    "    dataset = ReutersDataset(\n",
    "        df=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids_and_mask': [{'input_ids': tensor([[  101,  2624,  3799,  1010,  5553,  1019,  1006, 26665,  1007,  1011,\n",
       "             9130,  4297,  3734,  1037,  2194,  2008,  3084,  2376,  5038,  2974,\n",
       "             2005,  4929,  3085,  5733,  1998,  4274,  1011,  4198, 22449,  1010,\n",
       "             1996,   102],\n",
       "           [  101,  2624,  3799,  1006, 26665,  1007,  1011,  1062,  6038,  3654,\n",
       "             4297,  5766,  2928,  9231,  7874,  2056,  9857,  2002,  3464, 15705,\n",
       "             1997, 19920,  2004,  4600,  1999,  4684,  2399,  2004,  2002,  2038,\n",
       "             1999,   102],\n",
       "           [  101,  2047,  2259,  1010,  5553,  1022,  1006, 26665,  1007,  1011,\n",
       "             1057,  1012,  1055,  1012,  7027, 13753,  5157,  3333,  2058,  1996,\n",
       "             2197,  2048,  3134,  2004, 10216,  7597,  3123,  1010,  3040, 11522,\n",
       "             2056,   102],\n",
       "           [  101,  1008,  2117,  1011,  4284, 16565,  1013,  3745,  1002,  1014,\n",
       "             1012,  6640,  5443,  9765,  1002,  1014,  1012,  6421,  1008,  4341,\n",
       "             2039,  2260,  1012,  1023,  7473,  2102,  2000,  1002, 13539,  1012,\n",
       "             1017,   102]]),\n",
       "   'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "            1, 1, 1, 1, 1, 1, 1, 1],\n",
       "           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "            1, 1, 1, 1, 1, 1, 1, 1],\n",
       "           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "            1, 1, 1, 1, 1, 1, 1, 1],\n",
       "           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "            1, 1, 1, 1, 1, 1, 1, 1]])}],\n",
       " 'target': tensor([1, 0, 1, 0])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(train, tokenizer, max_len=32, batch_size=4)\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built!\n"
     ]
    }
   ],
   "source": [
    "class ReutersClassifier(nn.Module):\n",
    "    def __init__(self, n_classes, p=0.25):\n",
    "        super(ReutersClassifier, self).__init__()\n",
    "        self.bert_layer = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.classifier = nn.Linear(self.bert_layer.config.dim, n_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled_output = self.bert_layer(\n",
    "            input_ids=input_ids, \n",
    "            attention_mask=attention_mask)\n",
    "        main = self.dropout(pooled_output[0][:, 0, :])\n",
    "        return F.sigmoid(self.classifier(main))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = ReutersClassifier(n_classes=1)\n",
    "model.to(device)\n",
    "print(\"Model built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3549]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilty = model(sample[\"input_ids\"].to(device), sample[\"attention_mask\"].to(device))\n",
    "probabilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built!\n"
     ]
    }
   ],
   "source": [
    "class ReutersLinearClassifierV2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, p):\n",
    "        super(ReutersLinearClassifierV2, self).__init__()\n",
    "        self.PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "        self.distilbert_layer = AutoModel.from_pretrained(self.PRE_TRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.fc_dropout = nn.Dropout(p=p)\n",
    "        self.fc = nn.Linear(self.distilbert_layer.config.dim, self.distilbert_layer.config.dim)\n",
    "        self.classifier = nn.Linear(self.distilbert_layer.config.dim, n_classes)\n",
    "\n",
    "    def forward(self, ids_and_mask):\n",
    "        pool_list = []\n",
    "        for enc in ids_and_mask:\n",
    "            pooled_output = self.distilbert_layer(\n",
    "                input_ids=enc[\"input_ids\"],\n",
    "                attention_mask=enc[\"attention_mask\"])\n",
    "            branch = self.dropout(pooled_output[0][:, 0, :])\n",
    "            pool_list.append(branch)\n",
    "        init = torch.zeros(self.distilbert_layer.config.dim).to(device)\n",
    "        for branch in pool_list:\n",
    "            concat = torch.add(init, branch)\n",
    "        concat = self.fc(concat)\n",
    "        return self.classifier(self.fc_dropout(F.relu(concat)))\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.distilbert_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.distilbert_layer.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = ReutersLinearClassifierV2(n_classes=2, p=0.1)\n",
    "model.to(device)\n",
    "print(\"Model built!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2829c739df4e24be2afa1e71d85a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(tqdm(train_dataloader)):\n",
    "    for d in data[\"ids_and_mask\"]:\n",
    "        d[\"input_ids\"] = d[\"input_ids\"].to(device)\n",
    "        d[\"attention_mask\"] = d[\"attention_mask\"].to(device)\n",
    "    targets = data[\"target\"].to(device)\n",
    "\n",
    "    outputs = model(data[\"ids_and_mask\"])\n",
    "    outputs = F.softmax(outputs)\n",
    "    _, preds = torch.max(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5026, 0.4974]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
