{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0716 14:14:55.108985  4752 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I0716 14:14:59.116026  4752 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0716 14:14:59.118988  4752 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0716 14:15:00.081022  4752 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import joblib\n",
    "import config\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from model import ReutersClassifier\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from preprocessor import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = joblib.load(\"../data/train.bin\")\n",
    "valid = joblib.load(\"../data/valid.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 1 News</th>\n",
       "      <th>Top 2 News</th>\n",
       "      <th>Top 3 News</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-08-16</th>\n",
       "      <td>life maladroid</td>\n",
       "      <td>google dials pocketing motorola</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.44373</td>\n",
       "      <td>1.44664</td>\n",
       "      <td>1.43540</td>\n",
       "      <td>1.43804</td>\n",
       "      <td>265049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-08-26</th>\n",
       "      <td>facebook facing music  google calling tune?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43878</td>\n",
       "      <td>1.45012</td>\n",
       "      <td>1.43303</td>\n",
       "      <td>1.44985</td>\n",
       "      <td>215354</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-08</th>\n",
       "      <td>google want zagat?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.40803</td>\n",
       "      <td>1.40887</td>\n",
       "      <td>1.38733</td>\n",
       "      <td>1.38891</td>\n",
       "      <td>231356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-13</th>\n",
       "      <td>tech wrap: intel google launch android partner...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36381</td>\n",
       "      <td>1.37386</td>\n",
       "      <td>1.35585</td>\n",
       "      <td>1.36819</td>\n",
       "      <td>289282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-09-14</th>\n",
       "      <td>facebook google: say circles, say smart lists</td>\n",
       "      <td>connecting facebook, friendship longer required</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36819</td>\n",
       "      <td>1.37816</td>\n",
       "      <td>1.35907</td>\n",
       "      <td>1.37478</td>\n",
       "      <td>281384</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Top 1 News  \\\n",
       "date                                                            \n",
       "2011-08-16                                     life maladroid   \n",
       "2011-08-26        facebook facing music  google calling tune?   \n",
       "2011-09-08                                 google want zagat?   \n",
       "2011-09-13  tech wrap: intel google launch android partner...   \n",
       "2011-09-14      facebook google: say circles, say smart lists   \n",
       "\n",
       "                                                 Top 2 News Top 3 News  \\\n",
       "date                                                                     \n",
       "2011-08-16                  google dials pocketing motorola        NaN   \n",
       "2011-08-26                                              NaN        NaN   \n",
       "2011-09-08                                              NaN        NaN   \n",
       "2011-09-13                                              NaN        NaN   \n",
       "2011-09-14  connecting facebook, friendship longer required        NaN   \n",
       "\n",
       "               Open     High      Low    Close  Volume  label  \n",
       "date                                                           \n",
       "2011-08-16  1.44373  1.44664  1.43540  1.43804  265049      0  \n",
       "2011-08-26  1.43878  1.45012  1.43303  1.44985  215354      1  \n",
       "2011-09-08  1.40803  1.40887  1.38733  1.38891  231356      0  \n",
       "2011-09-13  1.36381  1.37386  1.35585  1.36819  289282      1  \n",
       "2011-09-14  1.36819  1.37816  1.35907  1.37478  281384      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, top_k):\n",
    "        self.df = df\n",
    "        self.targets = df.label.values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.top_k = top_k\n",
    "        self.len = len(self.df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        total_top = self.df.iloc[item, 0:self.top_k].values\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        enc_list = []\n",
    "        for k in total_top:\n",
    "            enc = self.tokenizer.encode_plus(\n",
    "                str(k),\n",
    "                max_length=self.max_len,\n",
    "                add_special_tokens=True,\n",
    "                return_token_type_ids=False,\n",
    "                pad_to_max_length=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors=\"pt\")\n",
    "            enc[\"input_ids\"] = enc[\"input_ids\"].flatten()\n",
    "            enc[\"attention_mask\"] = enc[\"attention_mask\"].flatten()\n",
    "            enc_list.append(enc)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"ids_and_mask\": enc_list,\n",
    "            \"target\": torch.tensor(target)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(df, tokenizer, max_len, top_k, batch_size):\n",
    "    dataset = ReutersDataset(\n",
    "        df=df,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len, \n",
    "        top_k=top_k)\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReutersClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, top_k, p=0.25):\n",
    "        super(ReutersClassifier, self).__init__()\n",
    "        self.PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "        self.distilbert_layer = AutoModel.from_pretrained(self.PRE_TRAINED_MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(p=p)\n",
    "        self.classifier = nn.Linear(self.distilbert_layer.config.dim*top_k, n_classes)\n",
    "\n",
    "    def forward(self, ids_and_mask):\n",
    "        pool_list = []\n",
    "        for enc in ids_and_mask:\n",
    "            pooled_output = self.distilbert_layer(\n",
    "                input_ids=enc[\"input_ids\"],\n",
    "                attention_mask=enc[\"attention_mask\"])\n",
    "            branch = self.dropout(pooled_output[0][:, 0, :])\n",
    "            pool_list.append(branch)\n",
    "        main = torch.cat([br for br in pool_list], 1)\n",
    "        return F.softmax(self.classifier(main))\n",
    "\n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.distilbert_layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.distilbert_layer.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0716 14:15:02.702307  4752 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0716 14:15:02.704278  4752 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0716 14:15:03.589309  4752 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0716 14:15:04.580305  4752 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0716 14:15:04.581281  4752 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0716 14:15:05.016308  4752 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/distilbert-base-uncased-pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ReutersClassifier(\n",
       "  (distilbert_layer): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (classifier): Linear(in_features=2304, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "model = ReutersClassifier(n_classes=2, top_k=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader = create_dataloader(train, tokenizer, max_len=32, top_k=3, batch_size=8)\n",
    "# for data in data_loader:\n",
    "#     for d in data[\"ids_and_mask\"]:\n",
    "#         d[\"input_ids\"] = d[\"input_ids\"].to(device)\n",
    "#         d[\"attention_mask\"] = d[\"attention_mask\"].to(device)\n",
    "#     targets = data[\"target\"].to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(data[\"ids_and_mask\"])\n",
    "#     print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matthews_correlation_coefficient(true_pos, true_neg, false_pos, false_neg):\n",
    "    nominator = (true_pos*true_neg-false_pos*false_neg)\n",
    "    denominator = np.sqrt((true_pos+false_pos)*(true_pos+false_neg)*(true_neg+false_pos)*(true_neg+false_neg)) + 1e-7\n",
    "    return (nominator / denominator)\n",
    "\n",
    "def train_distilbert(model, data_loader, loss_function, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        for d in data[\"ids_and_mask\"]:\n",
    "            d[\"input_ids\"] = d[\"input_ids\"].to(device)\n",
    "            d[\"attention_mask\"] = d[\"attention_mask\"].to(device)\n",
    "        targets = data[\"target\"].to(device)\n",
    "\n",
    "        outputs = model(data[\"ids_and_mask\"])\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        for p, t in zip(preds, targets):\n",
    "            if p == 1 and t == 1:\n",
    "                true_pos += 1\n",
    "            if p == 0 and t == 0:\n",
    "                true_neg += 1\n",
    "            if p == 1 and t == 0:\n",
    "                false_pos += 1\n",
    "            if p == 0 and t == 1:\n",
    "                false_neg += 1\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for d in data[\"ids_and_mask\"]:\n",
    "            d[\"input_ids\"] = d[\"input_ids\"].to(\"cpu\")\n",
    "            d[\"attention_mask\"] = d[\"attention_mask\"].to(\"cpu\")\n",
    "        targets = data[\"target\"].to(\"cpu\")\n",
    "\n",
    "    recall = float(true_pos) / float(true_pos + false_neg + 1e-7)\n",
    "    precision = float(true_pos) / float(true_pos + false_pos + 1e-7)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    accuracy = (true_pos + true_neg) / float(n_examples)\n",
    "    mcc = matthews_correlation_coefficient(true_pos, true_neg, false_pos, false_neg)\n",
    "\n",
    "    return accuracy, f1, mcc, np.mean(losses)\n",
    "\n",
    "\n",
    "def eval_distilbert(model, data_loader, loss_function, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            for d in data[\"ids_and_mask\"]:\n",
    "                d[\"input_ids\"] = d[\"input_ids\"].to(device)\n",
    "                d[\"attention_mask\"] = d[\"attention_mask\"].to(device)\n",
    "            targets = data[\"target\"].to(device)\n",
    "\n",
    "            outputs = model(data[\"ids_and_mask\"])\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            for p, t in zip(preds, targets):\n",
    "                if p == 1 and t == 1:\n",
    "                    true_pos += 1\n",
    "                if p == 0 and t == 0:\n",
    "                    true_neg += 1\n",
    "                if p == 1 and t == 0:\n",
    "                    false_pos += 1\n",
    "                if p == 0 and t == 1:\n",
    "                    false_neg += 1\n",
    "                    \n",
    "            for d in data[\"ids_and_mask\"]:\n",
    "                d[\"input_ids\"] = d[\"input_ids\"].to(\"cpu\")\n",
    "                d[\"attention_mask\"] = d[\"attention_mask\"].to(\"cpu\")\n",
    "            targets = data[\"target\"].to(\"cpu\")\n",
    "\n",
    "    recall = float(true_pos) / float(true_pos + false_neg + 1e-7)\n",
    "    precision = float(true_pos) / float(true_pos + false_pos + 1e-7)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    accuracy = (true_pos + true_neg) / float(n_examples)\n",
    "    mcc = matthews_correlation_coefficient(true_pos, true_neg, false_pos, false_neg)\n",
    "\n",
    "    return accuracy, f1, mcc, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Epoch 1/2\n",
      "Train | Loss: 0.7025 | Accuracy: 0.4963 | F1: 0.5098 | MCC: -0.0063\n",
      "Valid | Loss: 0.6968 | Accuracy: 0.4967 | F1: 0.0000 | MCC: 0.0000\n",
      "====================\n",
      "Epoch 2/2\n",
      "Train | Loss: 0.6940 | Accuracy: 0.5207 | F1: 0.3808 | MCC: 0.0383\n",
      "Valid | Loss: 0.6929 | Accuracy: 0.4915 | F1: 0.0251 | MCC: -0.0400\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = create_dataloader(train, tokenizer, max_len=32, top_k=3, batch_size=8)\n",
    "valid_dataloader = create_dataloader(valid, tokenizer, max_len=32, top_k=3, batch_size=8)\n",
    "\n",
    "EPOCH = 2\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.417, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * EPOCH\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps)\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, EPOCH))\n",
    "\n",
    "    train_acc, train_f1, train_mcc, train_loss = train_distilbert(\n",
    "        model, train_dataloader, loss_function, optimizer, device, scheduler, len(train))\n",
    "\n",
    "    print(\"Train | Loss: {:.4f} | Accuracy: {:.4f} | F1: {:.4f} | MCC: {:.4f}\".format(\n",
    "        train_loss, train_acc, train_f1, train_mcc))\n",
    "\n",
    "    val_acc, val_f1, val_mcc, val_loss = eval_distilbert(\n",
    "        model, valid_dataloader, loss_function, device, len(valid))\n",
    "\n",
    "    print(\"Valid | Loss: {:.4f} | Accuracy: {:.4f} | F1: {:.4f} | MCC: {:.4f}\".format(\n",
    "        val_loss, val_acc, val_f1, val_mcc))\n",
    "\n",
    "    history[\"train_f1\"].append(train_f1)\n",
    "    history[\"train_mcc\"].append(train_mcc)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_f1\"].append(val_f1)\n",
    "    history[\"val_mcc\"].append(val_mcc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"../weights/distilbert_test.bin\")\n",
    "        best_f1 = val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "def load_stock(ticker_name, start_date):\n",
    "    ticker = yf.Ticker(ticker_name)\n",
    "    hist = ticker.history(period=\"max\", start=start_date)\n",
    "    hist.index = hist.index.set_names(['date'])\n",
    "    hist = hist.reset_index(drop=False, inplace=False)\n",
    "    hist[\"date\"] = pd.to_datetime(hist[\"date\"], utc=True)\n",
    "    hist['date'] = hist['date'].apply(lambda x: x.date())\n",
    "    hist.sort_values(by='date', inplace=True)\n",
    "    hist.reset_index(drop=True, inplace=True)\n",
    "    hist[\"label\"] = hist[\"Close\"].diff(periods=1)\n",
    "    hist.dropna(inplace=True)\n",
    "    hist[\"label\"] = hist[\"label\"].map(lambda x: 1 if float(x) >= 0 else 0)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>22.00</td>\n",
       "      <td>22.53</td>\n",
       "      <td>21.97</td>\n",
       "      <td>22.48</td>\n",
       "      <td>80516100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>22.46</td>\n",
       "      <td>22.75</td>\n",
       "      <td>22.39</td>\n",
       "      <td>22.71</td>\n",
       "      <td>56081400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>22.58</td>\n",
       "      <td>23.12</td>\n",
       "      <td>22.58</td>\n",
       "      <td>23.06</td>\n",
       "      <td>99455500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>23.01</td>\n",
       "      <td>23.05</td>\n",
       "      <td>22.74</td>\n",
       "      <td>22.76</td>\n",
       "      <td>59706800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>22.91</td>\n",
       "      <td>23.09</td>\n",
       "      <td>22.76</td>\n",
       "      <td>22.84</td>\n",
       "      <td>60014400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>2020-07-09</td>\n",
       "      <td>216.33</td>\n",
       "      <td>216.38</td>\n",
       "      <td>211.47</td>\n",
       "      <td>214.32</td>\n",
       "      <td>33121700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>2020-07-10</td>\n",
       "      <td>213.62</td>\n",
       "      <td>214.08</td>\n",
       "      <td>211.08</td>\n",
       "      <td>213.67</td>\n",
       "      <td>26177600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>214.48</td>\n",
       "      <td>215.80</td>\n",
       "      <td>206.50</td>\n",
       "      <td>207.07</td>\n",
       "      <td>38135600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>2020-07-14</td>\n",
       "      <td>206.13</td>\n",
       "      <td>208.85</td>\n",
       "      <td>202.03</td>\n",
       "      <td>208.35</td>\n",
       "      <td>37591800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>2020-07-15</td>\n",
       "      <td>209.56</td>\n",
       "      <td>211.33</td>\n",
       "      <td>205.03</td>\n",
       "      <td>208.04</td>\n",
       "      <td>32144500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2146 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date    Open    High     Low   Close    Volume  Dividends  \\\n",
       "1     2012-01-04   22.00   22.53   21.97   22.48  80516100        0.0   \n",
       "2     2012-01-05   22.46   22.75   22.39   22.71  56081400        0.0   \n",
       "3     2012-01-06   22.58   23.12   22.58   23.06  99455500        0.0   \n",
       "4     2012-01-09   23.01   23.05   22.74   22.76  59706800        0.0   \n",
       "5     2012-01-10   22.91   23.09   22.76   22.84  60014400        0.0   \n",
       "...          ...     ...     ...     ...     ...       ...        ...   \n",
       "2142  2020-07-09  216.33  216.38  211.47  214.32  33121700        0.0   \n",
       "2143  2020-07-10  213.62  214.08  211.08  213.67  26177600        0.0   \n",
       "2144  2020-07-13  214.48  215.80  206.50  207.07  38135600        0.0   \n",
       "2145  2020-07-14  206.13  208.85  202.03  208.35  37591800        0.0   \n",
       "2146  2020-07-15  209.56  211.33  205.03  208.04  32144500        0.0   \n",
       "\n",
       "      Stock Splits  label  \n",
       "1                0      1  \n",
       "2                0      1  \n",
       "3                0      1  \n",
       "4                0      0  \n",
       "5                0      1  \n",
       "...            ...    ...  \n",
       "2142             0      1  \n",
       "2143             0      0  \n",
       "2144             0      0  \n",
       "2145             0      1  \n",
       "2146             0      0  \n",
       "\n",
       "[2146 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = load_stock(\"MSFT\", start_date=\"2012-01-01\")\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MMM',\n",
       " 'ABT',\n",
       " 'ABBV',\n",
       " 'ABMD',\n",
       " 'ACN',\n",
       " 'ATVI',\n",
       " 'ADBE',\n",
       " 'AMD',\n",
       " 'AAP',\n",
       " 'AES',\n",
       " 'AFL',\n",
       " 'A',\n",
       " 'APD',\n",
       " 'AKAM',\n",
       " 'ALK',\n",
       " 'ALB',\n",
       " 'ARE',\n",
       " 'ALXN',\n",
       " 'ALGN',\n",
       " 'ALLE',\n",
       " 'ADS',\n",
       " 'LNT',\n",
       " 'ALL',\n",
       " 'GOOGL',\n",
       " 'GOOG',\n",
       " 'MO',\n",
       " 'AMZN',\n",
       " 'AMCR',\n",
       " 'AEE',\n",
       " 'AAL',\n",
       " 'AEP',\n",
       " 'AXP',\n",
       " 'AIG',\n",
       " 'AMT',\n",
       " 'AWK',\n",
       " 'AMP',\n",
       " 'ABC',\n",
       " 'AME',\n",
       " 'AMGN',\n",
       " 'APH',\n",
       " 'ADI',\n",
       " 'ANSS',\n",
       " 'ANTM',\n",
       " 'AON',\n",
       " 'AOS',\n",
       " 'APA',\n",
       " 'AIV',\n",
       " 'AAPL',\n",
       " 'AMAT',\n",
       " 'APTV',\n",
       " 'ADM',\n",
       " 'ANET',\n",
       " 'AJG',\n",
       " 'AIZ',\n",
       " 'T',\n",
       " 'ATO',\n",
       " 'ADSK',\n",
       " 'ADP',\n",
       " 'AZO',\n",
       " 'AVB',\n",
       " 'AVY',\n",
       " 'BKR',\n",
       " 'BLL',\n",
       " 'BAC',\n",
       " 'BK',\n",
       " 'BAX',\n",
       " 'BDX',\n",
       " 'BRK.B',\n",
       " 'BBY',\n",
       " 'BIIB',\n",
       " 'BLK',\n",
       " 'BA',\n",
       " 'BKNG',\n",
       " 'BWA',\n",
       " 'BXP',\n",
       " 'BSX',\n",
       " 'BMY',\n",
       " 'AVGO',\n",
       " 'BR',\n",
       " 'BF.B',\n",
       " 'CHRW',\n",
       " 'COG',\n",
       " 'CDNS',\n",
       " 'CPB',\n",
       " 'COF',\n",
       " 'CAH',\n",
       " 'KMX',\n",
       " 'CCL',\n",
       " 'CARR',\n",
       " 'CAT',\n",
       " 'CBOE',\n",
       " 'CBRE',\n",
       " 'CDW',\n",
       " 'CE',\n",
       " 'CNC',\n",
       " 'CNP',\n",
       " 'CTL',\n",
       " 'CERN',\n",
       " 'CF',\n",
       " 'SCHW',\n",
       " 'CHTR',\n",
       " 'CVX',\n",
       " 'CMG',\n",
       " 'CB',\n",
       " 'CHD',\n",
       " 'CI',\n",
       " 'CINF',\n",
       " 'CTAS',\n",
       " 'CSCO',\n",
       " 'C',\n",
       " 'CFG',\n",
       " 'CTXS',\n",
       " 'CLX',\n",
       " 'CME',\n",
       " 'CMS',\n",
       " 'KO',\n",
       " 'CTSH',\n",
       " 'CL',\n",
       " 'CMCSA',\n",
       " 'CMA',\n",
       " 'CAG',\n",
       " 'CXO',\n",
       " 'COP',\n",
       " 'ED',\n",
       " 'STZ',\n",
       " 'COO',\n",
       " 'CPRT',\n",
       " 'GLW',\n",
       " 'CTVA',\n",
       " 'COST',\n",
       " 'COTY',\n",
       " 'CCI',\n",
       " 'CSX',\n",
       " 'CMI',\n",
       " 'CVS',\n",
       " 'DHI',\n",
       " 'DHR',\n",
       " 'DRI',\n",
       " 'DVA',\n",
       " 'DE',\n",
       " 'DAL',\n",
       " 'XRAY',\n",
       " 'DVN',\n",
       " 'DXCM',\n",
       " 'FANG',\n",
       " 'DLR',\n",
       " 'DFS',\n",
       " 'DISCA',\n",
       " 'DISCK',\n",
       " 'DISH',\n",
       " 'DG',\n",
       " 'DLTR',\n",
       " 'D',\n",
       " 'DPZ',\n",
       " 'DOV',\n",
       " 'DOW',\n",
       " 'DTE',\n",
       " 'DUK',\n",
       " 'DRE',\n",
       " 'DD',\n",
       " 'DXC',\n",
       " 'ETFC',\n",
       " 'EMN',\n",
       " 'ETN',\n",
       " 'EBAY',\n",
       " 'ECL',\n",
       " 'EIX',\n",
       " 'EW',\n",
       " 'EA',\n",
       " 'EMR',\n",
       " 'ETR',\n",
       " 'EOG',\n",
       " 'EFX',\n",
       " 'EQIX',\n",
       " 'EQR',\n",
       " 'ESS',\n",
       " 'EL',\n",
       " 'EVRG',\n",
       " 'ES',\n",
       " 'RE',\n",
       " 'EXC',\n",
       " 'EXPE',\n",
       " 'EXPD',\n",
       " 'EXR',\n",
       " 'XOM',\n",
       " 'FFIV',\n",
       " 'FB',\n",
       " 'FAST',\n",
       " 'FRT',\n",
       " 'FDX',\n",
       " 'FIS',\n",
       " 'FITB',\n",
       " 'FE',\n",
       " 'FRC',\n",
       " 'FISV',\n",
       " 'FLT',\n",
       " 'FLIR',\n",
       " 'FLS',\n",
       " 'FMC',\n",
       " 'F',\n",
       " 'FTNT',\n",
       " 'FTV',\n",
       " 'FBHS',\n",
       " 'FOXA',\n",
       " 'FOX',\n",
       " 'BEN',\n",
       " 'FCX',\n",
       " 'GPS',\n",
       " 'GRMN',\n",
       " 'IT',\n",
       " 'GD',\n",
       " 'GE',\n",
       " 'GIS',\n",
       " 'GM',\n",
       " 'GPC',\n",
       " 'GILD',\n",
       " 'GL',\n",
       " 'GPN',\n",
       " 'GS',\n",
       " 'GWW',\n",
       " 'HRB',\n",
       " 'HAL',\n",
       " 'HBI',\n",
       " 'HOG',\n",
       " 'HIG',\n",
       " 'HAS',\n",
       " 'HCA',\n",
       " 'PEAK',\n",
       " 'HSIC',\n",
       " 'HSY',\n",
       " 'HES',\n",
       " 'HPE',\n",
       " 'HLT',\n",
       " 'HFC',\n",
       " 'HOLX',\n",
       " 'HD',\n",
       " 'HON',\n",
       " 'HRL',\n",
       " 'HST',\n",
       " 'HWM',\n",
       " 'HPQ',\n",
       " 'HUM',\n",
       " 'HBAN',\n",
       " 'HII',\n",
       " 'IEX',\n",
       " 'IDXX',\n",
       " 'INFO',\n",
       " 'ITW',\n",
       " 'ILMN',\n",
       " 'INCY',\n",
       " 'IR',\n",
       " 'INTC',\n",
       " 'ICE',\n",
       " 'IBM',\n",
       " 'IP',\n",
       " 'IPG',\n",
       " 'IFF',\n",
       " 'INTU',\n",
       " 'ISRG',\n",
       " 'IVZ',\n",
       " 'IPGP',\n",
       " 'IQV',\n",
       " 'IRM',\n",
       " 'JKHY',\n",
       " 'J',\n",
       " 'JBHT',\n",
       " 'SJM',\n",
       " 'JNJ',\n",
       " 'JCI',\n",
       " 'JPM',\n",
       " 'JNPR',\n",
       " 'KSU',\n",
       " 'K',\n",
       " 'KEY',\n",
       " 'KEYS',\n",
       " 'KMB',\n",
       " 'KIM',\n",
       " 'KMI',\n",
       " 'KLAC',\n",
       " 'KSS',\n",
       " 'KHC',\n",
       " 'KR',\n",
       " 'LB',\n",
       " 'LHX',\n",
       " 'LH',\n",
       " 'LRCX',\n",
       " 'LW',\n",
       " 'LVS',\n",
       " 'LEG',\n",
       " 'LDOS',\n",
       " 'LEN',\n",
       " 'LLY',\n",
       " 'LNC',\n",
       " 'LIN',\n",
       " 'LYV',\n",
       " 'LKQ',\n",
       " 'LMT',\n",
       " 'L',\n",
       " 'LOW',\n",
       " 'LYB',\n",
       " 'MTB',\n",
       " 'MRO',\n",
       " 'MPC',\n",
       " 'MKTX',\n",
       " 'MAR',\n",
       " 'MMC',\n",
       " 'MLM',\n",
       " 'MAS',\n",
       " 'MA',\n",
       " 'MKC',\n",
       " 'MXIM',\n",
       " 'MCD',\n",
       " 'MCK',\n",
       " 'MDT',\n",
       " 'MRK',\n",
       " 'MET',\n",
       " 'MTD',\n",
       " 'MGM',\n",
       " 'MCHP',\n",
       " 'MU',\n",
       " 'MSFT',\n",
       " 'MAA',\n",
       " 'MHK',\n",
       " 'TAP',\n",
       " 'MDLZ',\n",
       " 'MNST',\n",
       " 'MCO',\n",
       " 'MS',\n",
       " 'MOS',\n",
       " 'MSI',\n",
       " 'MSCI',\n",
       " 'MYL',\n",
       " 'NDAQ',\n",
       " 'NOV',\n",
       " 'NTAP',\n",
       " 'NFLX',\n",
       " 'NWL',\n",
       " 'NEM',\n",
       " 'NWSA',\n",
       " 'NWS',\n",
       " 'NEE',\n",
       " 'NLSN',\n",
       " 'NKE',\n",
       " 'NI',\n",
       " 'NBL',\n",
       " 'JWN',\n",
       " 'NSC',\n",
       " 'NTRS',\n",
       " 'NOC',\n",
       " 'NLOK',\n",
       " 'NCLH',\n",
       " 'NRG',\n",
       " 'NUE',\n",
       " 'NVDA',\n",
       " 'NVR',\n",
       " 'ORLY',\n",
       " 'OXY',\n",
       " 'ODFL',\n",
       " 'OMC',\n",
       " 'OKE',\n",
       " 'ORCL',\n",
       " 'OTIS',\n",
       " 'PCAR',\n",
       " 'PKG',\n",
       " 'PH',\n",
       " 'PAYX',\n",
       " 'PAYC',\n",
       " 'PYPL',\n",
       " 'PNR',\n",
       " 'PBCT',\n",
       " 'PEP',\n",
       " 'PKI',\n",
       " 'PRGO',\n",
       " 'PFE',\n",
       " 'PM',\n",
       " 'PSX',\n",
       " 'PNW',\n",
       " 'PXD',\n",
       " 'PNC',\n",
       " 'PPG',\n",
       " 'PPL',\n",
       " 'PFG',\n",
       " 'PG',\n",
       " 'PGR',\n",
       " 'PLD',\n",
       " 'PRU',\n",
       " 'PEG',\n",
       " 'PSA',\n",
       " 'PHM',\n",
       " 'PVH',\n",
       " 'QRVO',\n",
       " 'PWR',\n",
       " 'QCOM',\n",
       " 'DGX',\n",
       " 'RL',\n",
       " 'RJF',\n",
       " 'RTX',\n",
       " 'O',\n",
       " 'REG',\n",
       " 'REGN',\n",
       " 'RF',\n",
       " 'RSG',\n",
       " 'RMD',\n",
       " 'RHI',\n",
       " 'ROK',\n",
       " 'ROL',\n",
       " 'ROP',\n",
       " 'ROST',\n",
       " 'RCL',\n",
       " 'SPGI',\n",
       " 'CRM',\n",
       " 'SBAC',\n",
       " 'SLB',\n",
       " 'STX',\n",
       " 'SEE',\n",
       " 'SRE',\n",
       " 'NOW',\n",
       " 'SHW',\n",
       " 'SPG',\n",
       " 'SWKS',\n",
       " 'SLG',\n",
       " 'SNA',\n",
       " 'SO',\n",
       " 'LUV',\n",
       " 'SWK',\n",
       " 'SBUX',\n",
       " 'STT',\n",
       " 'STE',\n",
       " 'SYK',\n",
       " 'SIVB',\n",
       " 'SYF',\n",
       " 'SNPS',\n",
       " 'SYY',\n",
       " 'TMUS',\n",
       " 'TROW',\n",
       " 'TTWO',\n",
       " 'TPR',\n",
       " 'TGT',\n",
       " 'TEL',\n",
       " 'FTI',\n",
       " 'TFX',\n",
       " 'TXN',\n",
       " 'TXT',\n",
       " 'TMO',\n",
       " 'TIF',\n",
       " 'TJX',\n",
       " 'TSCO',\n",
       " 'TT',\n",
       " 'TDG',\n",
       " 'TRV',\n",
       " 'TFC',\n",
       " 'TWTR',\n",
       " 'TSN',\n",
       " 'UDR',\n",
       " 'ULTA',\n",
       " 'USB',\n",
       " 'UAA',\n",
       " 'UA',\n",
       " 'UNP',\n",
       " 'UAL',\n",
       " 'UNH',\n",
       " 'UPS',\n",
       " 'URI',\n",
       " 'UHS',\n",
       " 'UNM',\n",
       " 'VFC',\n",
       " 'VLO',\n",
       " 'VAR',\n",
       " 'VTR',\n",
       " 'VRSN',\n",
       " 'VRSK',\n",
       " 'VZ',\n",
       " 'VRTX',\n",
       " 'VIAC',\n",
       " 'V',\n",
       " 'VNO',\n",
       " 'VMC',\n",
       " 'WRB',\n",
       " 'WAB',\n",
       " 'WMT',\n",
       " 'WBA',\n",
       " 'DIS',\n",
       " 'WM',\n",
       " 'WAT',\n",
       " 'WEC',\n",
       " 'WFC',\n",
       " 'WELL',\n",
       " 'WST',\n",
       " 'WDC',\n",
       " 'WU',\n",
       " 'WRK',\n",
       " 'WY',\n",
       " 'WHR',\n",
       " 'WMB',\n",
       " 'WLTW',\n",
       " 'WYNN',\n",
       " 'XEL',\n",
       " 'XRX',\n",
       " 'XLNX',\n",
       " 'XYL',\n",
       " 'YUM',\n",
       " 'ZBRA',\n",
       " 'ZBH',\n",
       " 'ZION',\n",
       " 'ZTS']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500_file = \"../data/sp500tickers.pkl\"\n",
    "with open(sp500_file, 'rb') as f:\n",
    "    tickers = pickle.load(f)\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
