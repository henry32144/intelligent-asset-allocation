{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 14:49:13.561837 15980 file_utils.py:39] PyTorch version 1.2.0 available.\n",
      "I0721 14:49:19.043715 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c\n",
      "I0721 14:49:19.045713 15980 configuration_utils.py:321] Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:49:20.787230 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import logging\n",
    "import warnings\n",
    "import requests\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from zero_shot_learner import extend_df_with_cos_sim\n",
    "from preprocessor import NewsPreprocessor\n",
    "from preprocessor import transform_df\n",
    "from contractions import contractions_dict\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logger = logging.getLogger()\n",
    "logger.disabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock(ticker_name, start_date=\"2012-01-01\"):\n",
    "    ticker = yf.Ticker(ticker_name)\n",
    "    hist = ticker.history(period=\"max\", start=start_date)\n",
    "    hist.index = hist.index.set_names(['date'])\n",
    "    hist = hist.reset_index(drop=False, inplace=False)\n",
    "    hist[\"date\"] = pd.to_datetime(hist[\"date\"], utc=True)\n",
    "    hist['date'] = hist['date'].apply(lambda x: x.date())\n",
    "    hist.sort_values(by='date', inplace=True)\n",
    "    hist.reset_index(drop=True, inplace=True)\n",
    "    hist[\"label\"] = hist[\"Close\"].diff(periods=1)\n",
    "    hist.dropna(inplace=True)\n",
    "    hist[\"label\"] = hist[\"label\"].map(lambda x: 1 if float(x) >= 0 else 0)\n",
    "    return hist\n",
    "\n",
    "def load_news(df, labels, sort_by, k):\n",
    "    \"\"\"\n",
    "    :param file_name: str\n",
    "    :param labels: list of str (for zero-shot learner)\n",
    "    :param sort_by: str (str in labels)\n",
    "    :param k: int (top k news)\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    df.drop_duplicates(subset=\"title\", inplace=True)\n",
    "    preprocessor = NewsPreprocessor(contractions_dict=contractions_dict)\n",
    "    df[\"clean_title\"] = df[\"title\"].apply(lambda x: preprocessor.ultimate_clean(x))\n",
    "    df = extend_df_with_cos_sim(df=df, col=\"clean_title\", labels=labels, sort_by=sort_by)\n",
    "    df = transform_df(df=df, sort_by=sort_by, k=k)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_function(df, ticker, return_dict):\n",
    "    news = df[df[\"ticker\"] == str(ticker)]\n",
    "    news = load_news(news, labels=[\"stock\"], sort_by=\"stock\", k=5)\n",
    "    stock = load_stock(str(ticker), start_date=\"2012-01-01\")\n",
    "    news_and_stock = pd.merge(news, stock, on=[\"date\"])\n",
    "    news_and_stock.set_index('date', inplace=True)\n",
    "    return_dict[ticker] = news_and_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33c09fd7d1a4d91aa7308614a7cf4c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=99.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 14:49:49.498695 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:49:49.499730 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:49:49.500700 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 14:49:53.127459 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 14:49:53.128459 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 14:49:53.129460 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 14:49:53.131463 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 14:49:54.088213 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:49:54.090245 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:49:54.485790 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 14:55:38.667075 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:55:38.669073 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:55:38.670071 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 14:55:42.291977 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 14:55:42.292921 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 14:55:42.292921 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 14:55:42.293947 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 14:55:43.235400 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:55:43.238426 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:55:43.585233 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 14:57:47.968962 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:57:47.971965 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:57:47.972925 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 14:57:51.549889 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 14:57:51.551881 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 14:57:51.552954 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 14:57:51.555168 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 14:57:52.488153 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 14:57:52.489151 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 14:57:52.871978 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:03:34.172717 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:03:34.174709 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:03:34.175714 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:03:37.754577 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:03:37.755547 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:03:37.756549 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:03:37.756549 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:03:38.700679 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:03:38.702650 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:03:39.084773 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:08:57.784761 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:08:57.785731 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:08:57.785731 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:09:01.315955 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:09:01.316953 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:09:01.316953 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:09:01.317972 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:09:02.247016 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:09:02.249017 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:09:02.649835 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:11:14.862935 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:11:14.865939 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:11:14.866972 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:11:18.471983 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:11:18.472987 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:11:18.473953 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:11:18.473953 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:11:19.409722 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:11:19.411758 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:11:19.816775 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:17:07.660378 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:17:07.662362 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:17:07.663350 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:17:11.262400 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:17:11.265402 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:17:11.266393 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:17:11.268393 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:17:12.344316 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:17:12.347350 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:17:12.747479 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:19:29.057780 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:19:29.060813 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:19:29.063365 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:19:32.607300 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:19:32.608337 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:19:32.608337 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:19:32.609299 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:19:33.556392 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:19:33.558364 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:19:34.000878 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:20:52.327252 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:20:52.329225 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:20:52.331226 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:20:56.043455 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:20:56.044436 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:20:56.045439 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:20:56.045439 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:20:56.987140 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:20:56.991086 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:20:57.386008 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:21:19.007944 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:21:19.011953 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:21:19.013950 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:21:22.575784 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:21:22.576819 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:21:22.576819 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:21:22.577785 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:21:23.495182 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:21:23.497181 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:21:23.554198 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:27:28.189563 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:27:28.191570 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:27:28.192643 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:27:31.761910 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:27:31.763911 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:27:31.765876 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:27:31.766876 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:27:32.748828 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:27:32.749826 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:27:33.232703 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:28:03.885887 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:28:03.886890 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:28:03.887901 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:28:07.852941 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:28:07.853907 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:28:07.854904 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:28:07.856912 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:28:08.805620 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:28:08.807620 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:28:08.879831 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:29:22.562187 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:29:22.566158 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:29:22.568183 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:29:26.300645 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:29:26.301644 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:29:26.301644 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:29:26.303061 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:29:27.236739 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:29:27.237714 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:29:27.641093 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:30:39.654458 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:30:39.656423 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:30:39.657426 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:30:43.264407 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:30:43.266373 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:30:43.267374 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:30:43.269369 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:30:44.226859 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:30:44.229865 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:30:44.637112 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:31:43.393774 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:31:43.394763 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:31:43.395732 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:31:46.972807 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:31:46.975077 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:31:46.976077 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:31:46.977084 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:31:47.922039 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:31:47.924053 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:31:48.327969 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:32:06.664063 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:32:06.667062 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:32:06.668101 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:32:10.285852 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:32:10.287820 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:32:10.289809 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:32:10.291822 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:32:11.567097 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:32:11.570132 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:32:11.641690 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:33:34.355024 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:33:34.356994 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:33:34.362006 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:33:39.441399 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:33:39.443596 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:33:39.443596 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:33:39.445588 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:33:40.505274 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:33:40.506249 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:33:41.058469 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:35:24.066693 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:35:24.067664 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:35:24.068664 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:35:27.659789 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:35:27.660793 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:35:27.661790 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:35:27.662791 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:35:28.662271 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:35:28.663344 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:35:29.084831 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:35:55.534830 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:35:55.535796 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:35:55.536795 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:35:59.122717 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:35:59.123719 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:35:59.124725 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:35:59.125718 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:36:00.067724 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:36:00.068759 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:36:00.128755 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:36:51.463832 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:36:51.466848 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:36:51.467819 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:36:55.062799 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:36:55.063802 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:36:55.064767 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:36:55.065769 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:36:55.997829 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:36:55.998801 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:36:56.389715 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:38:32.777877 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:38:32.778877 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:38:32.780885 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:38:36.296262 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:38:36.297260 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:38:36.298262 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:38:36.299261 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:38:37.235692 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:38:37.236694 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:38:37.389771 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:39:34.488197 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:39:34.490195 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:39:34.490195 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:39:38.083909 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:39:38.084909 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:39:38.085910 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:39:38.087911 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:39:39.107226 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:39:39.108229 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:39:39.492115 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:43:13.064515 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:43:13.065507 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:43:13.065507 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:43:16.548503 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:43:16.549507 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:43:16.549507 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:43:16.550505 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:43:17.459506 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:43:17.460510 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:43:17.814511 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:44:53.056007 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:44:53.057008 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:44:53.058008 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:44:56.696008 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:44:56.697017 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:44:56.698010 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:44:56.699008 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:44:57.658008 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:44:57.659017 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:44:58.023010 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:45:29.296007 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:45:29.298008 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:45:29.299008 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:45:32.816010 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:45:32.817007 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:45:32.818009 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:45:32.818009 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:45:33.746009 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:45:33.748011 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:45:33.799007 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:46:27.846637 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:46:27.847637 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:46:27.848651 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:46:31.486638 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:46:31.487638 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:46:31.488641 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:46:31.488641 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:46:32.410640 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:46:32.412641 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:46:32.841639 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:48:54.559638 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:48:54.561674 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:48:54.561674 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:48:58.627672 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:48:58.629652 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:48:58.629652 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:48:58.630639 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:48:59.623639 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:48:59.624669 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:49:00.002642 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:54:29.255502 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:54:29.256471 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:54:29.257503 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:54:32.985082 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:54:32.986118 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:54:32.987086 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:54:32.988085 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:54:33.916461 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:54:33.917462 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:54:34.315338 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:55:50.739253 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:55:50.740254 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:55:50.741252 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:55:54.278822 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:55:54.280820 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:55:54.281824 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:55:54.281824 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:55:55.219278 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:55:55.220254 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:55:55.597620 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:57:35.738636 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:57:35.739641 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:57:35.740677 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:57:39.441116 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:57:39.442113 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:57:39.442673 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:57:39.443653 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:57:40.476895 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:57:40.477872 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:57:40.887342 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 15:58:58.478025 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:58:58.481027 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:58:58.484611 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 15:59:02.061969 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 15:59:02.062968 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 15:59:02.063969 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 15:59:02.064968 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 15:59:03.011495 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 15:59:03.012465 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 15:59:03.499532 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:00:11.431815 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:00:11.433116 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:00:11.434087 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:00:15.068274 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:00:15.069276 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:00:15.070245 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:00:15.071245 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:00:16.030899 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:00:16.032612 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:00:16.395408 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:00:57.144680 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:00:57.145652 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:00:57.145652 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:01:00.750556 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:01:00.752550 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:01:00.754550 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:01:00.756552 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:01:01.744543 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:01:01.746544 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:01:01.811639 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:01:51.267075 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:01:51.268046 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:01:51.269047 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:01:54.948169 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:01:54.949169 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:01:54.950141 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:01:54.951139 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:01:55.875612 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:01:55.877643 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:01:56.318424 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:02:25.271860 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:02:25.272890 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:02:25.273866 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:02:28.918698 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:02:28.919664 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:02:28.920697 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:02:28.921669 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:02:29.891416 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:02:29.892416 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:02:29.944963 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:03:50.655804 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:03:50.657804 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:03:50.657804 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:03:54.282639 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:03:54.283616 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:03:54.284614 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:03:54.285610 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:03:55.211637 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:03:55.213605 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:03:55.620438 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:04:07.834386 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:04:07.836379 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:04:07.836379 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:04:11.382147 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:04:11.382147 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:04:11.383469 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:04:11.384428 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:04:12.291447 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:04:12.294736 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:04:12.357698 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:06:36.970131 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:06:36.971157 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:06:36.972155 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:06:40.574785 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:06:40.575759 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:06:40.576757 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:06:40.577759 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:06:41.536925 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:06:41.538928 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:06:41.964787 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:07:09.701205 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:07:09.703150 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:07:09.704135 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:07:13.435770 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:07:13.438722 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:07:13.440731 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:07:13.442730 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:07:14.457490 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:07:14.459460 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:07:14.531959 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:08:50.172071 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:08:50.172921 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:08:50.173962 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:08:53.832005 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:08:53.832005 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:08:53.833055 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:08:53.834034 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:08:54.767554 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:08:54.769554 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:08:55.146921 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:10:22.567060 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:10:22.568092 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:10:22.569056 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:10:26.223424 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:10:26.224419 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:10:26.225418 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:10:26.225418 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:10:27.206649 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:10:27.207649 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:10:27.644088 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:10:53.547544 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:10:53.548541 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:10:53.549545 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:10:57.194532 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:10:57.195534 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:10:57.196532 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:10:57.196532 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:10:58.170321 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:10:58.171352 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:10:58.253237 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:11:07.855501 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:11:07.856500 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:11:07.857500 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:11:11.467718 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:11:11.470720 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:11:11.473889 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:11:11.474888 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:11:12.453414 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:11:12.454435 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:11:12.516627 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:11:25.568879 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:11:25.570780 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:11:25.574821 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:11:29.182415 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:11:29.183372 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:11:29.183372 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:11:29.185403 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:11:30.120315 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:11:30.122313 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:11:30.582430 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:12:26.056876 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:12:26.057872 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:12:26.058871 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:12:29.768565 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:12:29.770569 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:12:29.771566 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:12:29.777590 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:12:30.717498 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:12:30.718495 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:12:30.842490 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:12:37.883706 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:12:37.884674 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:12:37.885676 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:12:41.680647 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:12:41.683688 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:12:41.686689 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:12:41.687639 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:12:42.727203 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:12:42.729171 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:12:42.805269 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:14:06.517344 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:14:06.518313 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:14:06.519345 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:14:10.212817 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:14:10.213851 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:14:10.213851 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:14:10.214832 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:14:11.145573 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:14:11.146574 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:14:11.555782 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:14:52.173583 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:14:52.174561 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:14:52.175590 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:14:55.916796 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:14:55.919786 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:14:55.920830 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:14:55.922799 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:14:56.989272 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:14:56.991268 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:14:57.066626 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:15:05.247462 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:15:05.249547 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:15:05.251512 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:15:09.004850 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:15:09.005847 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:15:09.006820 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:15:09.007817 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:15:09.997956 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:15:09.999928 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:15:10.065780 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:15:39.050091 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:15:39.051091 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:15:39.052101 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:15:42.705130 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:15:42.706099 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:15:42.707100 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:15:42.707100 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:15:43.747031 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:15:43.748032 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:15:44.154198 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:16:15.439696 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:16:15.440668 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:16:15.441698 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:16:19.335184 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:16:19.337155 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:16:19.337155 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:16:19.338155 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:16:20.423522 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:16:20.424499 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:16:20.486468 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:16:49.757761 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:16:49.760727 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:16:49.764458 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:16:53.821723 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:16:53.822755 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:16:53.823759 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:16:53.824723 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:16:54.768286 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:16:54.770247 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:16:55.199281 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:18:22.450254 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:18:22.452255 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:18:22.453225 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:18:26.209329 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:18:26.210366 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:18:26.211328 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:18:26.211328 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:18:27.157366 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:18:27.158337 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:18:27.534235 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:24:33.935193 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:24:33.937192 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:24:33.937192 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:24:37.748027 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:24:37.749996 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:24:37.751990 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:24:37.754117 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:24:38.838476 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:24:38.839482 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:24:39.334152 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:25:39.076591 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:25:39.078592 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:25:39.078592 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:25:42.629627 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:25:42.631624 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:25:42.634625 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:25:42.636639 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:25:43.640389 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:25:43.643437 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:25:44.084488 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:31:20.124226 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:31:20.127224 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:31:20.128263 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:31:23.967384 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:31:23.968387 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:31:23.969383 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:31:23.970382 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:31:24.961904 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:31:24.964140 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:31:25.343608 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:33:50.511145 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:33:50.512177 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:33:50.513469 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:33:54.066801 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:33:54.067799 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:33:54.068773 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:33:54.069775 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:33:55.127581 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:33:55.128570 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:33:55.489915 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:35:21.368474 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:35:21.370507 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:35:21.371477 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:35:25.157530 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:35:25.161583 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:35:25.165267 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:35:25.170243 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:35:26.144893 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:35:26.145895 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:35:26.571422 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:35:44.158876 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:35:44.160872 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:35:44.160872 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:35:47.751818 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:35:47.755144 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:35:47.757110 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:35:47.759110 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:35:49.589393 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:35:49.590402 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:35:49.652354 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:35:55.515362 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:35:55.518322 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:35:55.520317 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:35:59.190550 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:35:59.191522 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:35:59.192522 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:35:59.192522 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:36:00.157995 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:00.158994 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:00.220016 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:36:06.313685 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:06.314704 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:06.315703 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:36:09.913059 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:36:09.914098 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:36:09.914098 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:36:09.915106 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:36:10.906964 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:10.908973 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:10.983277 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:36:18.652037 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:18.655088 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:18.657088 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:36:22.254137 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:36:22.255136 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:36:22.255136 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:36:22.256135 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:36:23.225022 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:23.226018 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:23.285145 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:36:28.610361 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:28.611332 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:28.612329 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:36:32.284742 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:36:32.285745 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:36:32.285745 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:36:32.286714 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:36:33.224453 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:33.225453 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:33.584660 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:36:42.895644 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:42.896613 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:42.897613 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:36:46.434172 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:36:46.435105 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:36:46.436104 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:36:46.436104 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:36:47.367524 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:47.368527 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:47.433513 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:36:56.368201 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:36:56.370153 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:36:56.372139 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:36:59.901551 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:36:59.902552 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:36:59.903548 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:36:59.904560 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:37:00.850054 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:00.851055 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:00.912603 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:37:06.931780 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:06.933619 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:06.934618 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:37:11.418477 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:37:11.419474 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:37:11.420474 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:37:11.421481 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:37:12.730792 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:12.732319 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:12.904190 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:37:19.211550 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:19.213558 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:19.215552 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:37:22.743493 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:37:22.744493 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:37:22.745493 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:37:22.746492 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:37:23.773461 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:23.775467 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:23.829460 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:37:29.186565 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:29.189568 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:29.190565 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:37:32.767391 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:37:32.769360 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:37:32.771364 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:37:32.774387 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:37:33.717780 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:33.720790 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:34.084782 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:37:40.824391 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:40.825397 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:40.826362 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:37:44.335515 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:37:44.336484 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:37:44.337485 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:37:44.337485 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:37:45.266309 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:45.269307 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:45.338484 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:37:58.427883 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:37:58.429884 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:37:58.430891 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:38:02.199038 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:38:02.200037 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:38:02.201067 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:38:02.202036 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:38:03.134528 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:38:03.135528 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:38:03.198541 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:38:49.877750 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:38:49.878747 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:38:49.879717 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:38:53.386313 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:38:53.387317 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:38:53.388287 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:38:53.388287 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:38:54.335544 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:38:54.336512 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:38:54.737332 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:38:59.783172 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:38:59.784146 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:38:59.785145 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:39:03.385389 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:39:03.386420 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:39:03.387391 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:39:03.388395 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:39:04.315850 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:39:04.317820 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:39:04.423751 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:40:57.473914 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:40:57.475948 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:40:57.477917 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:41:01.102323 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:41:01.102323 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:41:01.103408 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:41:01.104372 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:41:02.044516 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:41:02.045517 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:41:02.427673 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:41:38.939248 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:41:38.942209 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:41:38.944295 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:41:42.591624 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:41:42.592630 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:41:42.593623 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:41:42.594624 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:41:43.543134 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:41:43.545150 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:41:43.598486 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:44:44.229562 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:44:44.231586 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:44:44.232559 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:44:47.811010 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:44:47.811010 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:44:47.812725 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:44:47.812725 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:44:48.770311 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:44:48.771336 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:44:49.255458 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:45:02.807414 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:02.808414 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:02.809415 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:45:06.336919 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:45:06.337917 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:45:06.337917 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:45:06.338889 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:45:07.358390 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:07.360363 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:07.414722 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:45:19.007817 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:19.008817 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:19.009816 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:45:22.701889 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:45:22.704142 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:45:22.705176 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:45:22.707140 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:45:23.632314 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:23.633280 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:23.706065 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:45:41.721640 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:41.724641 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:41.726640 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:45:45.347652 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:45:45.349661 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:45:45.350648 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:45:45.352650 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:45:46.489211 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:46.490787 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:46.555877 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:45:58.117311 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:45:58.121317 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:45:58.123281 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:46:01.693292 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:46:01.694260 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:46:01.695261 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:46:01.696268 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:46:02.626429 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:46:02.627427 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:46:03.110529 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:47:04.325528 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:47:04.326498 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:47:04.327497 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:47:07.900779 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:47:07.902644 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:47:07.904637 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:47:07.905626 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:47:09.044504 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:47:09.045473 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:47:09.455095 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:50:08.856148 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:50:08.861115 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:50:08.864156 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:50:12.445724 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:50:12.445724 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:50:12.446724 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:50:12.447720 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:50:13.383199 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:50:13.385163 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:50:13.764760 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:50:34.487507 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:50:34.490469 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:50:34.492480 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:50:38.111666 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:50:38.113669 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:50:38.114664 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:50:38.115664 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:50:39.041489 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:50:39.043459 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:50:39.105403 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:51:40.208671 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:51:40.210668 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:51:40.210668 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:51:43.758643 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:51:43.759643 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:51:43.760616 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:51:43.761616 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:51:44.701052 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:51:44.704450 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:51:45.100590 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:51:53.930813 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:51:53.931778 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:51:53.932763 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:51:57.534578 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:51:57.537569 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:51:57.539572 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:51:57.542572 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:51:58.565014 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:51:58.565987 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:51:58.631037 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:52:03.705295 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:03.706295 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:03.707294 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:52:07.279183 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:52:07.280191 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:52:07.281181 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:52:07.281181 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:52:08.236997 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:08.239994 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:08.312839 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:52:12.685743 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:12.686712 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:12.687714 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:52:16.243535 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:52:16.244535 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:52:16.245510 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:52:16.246540 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:52:17.174412 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:17.175411 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:17.227407 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:52:22.878179 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:22.880177 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:22.882168 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:52:26.529920 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:52:26.531922 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:52:26.533351 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:52:26.534320 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:52:27.479079 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:27.480052 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:27.543788 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:52:42.849988 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:42.850989 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:42.852024 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:52:46.400403 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:52:46.401403 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:52:46.402407 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:52:46.403403 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:52:47.334110 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:47.335098 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:47.683213 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:52:55.708104 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:52:55.709109 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:52:55.710075 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:52:59.344948 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:52:59.345948 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:52:59.346981 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:52:59.347950 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:00.336107 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:00.338106 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:00.405854 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:53:05.365008 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:05.367008 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:05.368058 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:53:09.003435 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:53:09.004438 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:53:09.005416 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:53:09.005416 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:09.933720 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:09.936125 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:10.010431 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:53:17.091060 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:17.092061 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:17.094084 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:53:20.685684 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:53:20.687643 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:53:20.689645 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:53:20.692676 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:21.641196 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:21.642194 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:21.714620 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:53:26.815128 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:26.818126 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:26.820123 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:53:30.419892 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:53:30.420893 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:53:30.420893 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:53:30.421889 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:31.341977 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:31.344009 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:31.393932 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:53:36.003330 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:36.005331 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:36.006325 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:53:39.632615 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:53:39.633615 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:53:39.633615 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:53:39.634585 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:40.582395 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:40.585360 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:40.657866 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:53:44.743509 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:44.750509 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:44.752509 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:53:49.371877 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:53:49.372911 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:53:49.373939 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:53:49.374892 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:53:50.343172 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:53:50.345153 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:53:50.793570 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:54:10.811740 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:10.812711 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:10.813747 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:54:14.319741 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:54:14.320742 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:54:14.321741 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:54:14.322393 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:54:15.240668 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:15.241660 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:15.305721 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:54:35.838589 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:35.844055 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:35.846550 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:54:39.408808 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:54:39.409775 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:54:39.409775 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:54:39.410808 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:54:40.355000 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:40.356971 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:40.421648 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:54:48.045664 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:48.046635 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:48.047667 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:54:51.590533 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:54:51.591535 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:54:51.592536 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:54:51.593576 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:54:52.516501 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:52.517504 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:52.937541 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:54:57.770391 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:54:57.771424 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:54:57.771424 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:55:01.313879 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:55:01.314864 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:55:01.314864 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:55:01.315865 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:55:02.379702 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:55:02.381701 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:55:02.439702 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 16:55:07.697285 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:55:07.698144 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:55:07.699137 15980 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0721 16:55:11.271375 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0721 16:55:11.272375 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0721 16:55:11.274329 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0721 16:55:11.274431 15980 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0721 16:55:12.192895 15980 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0721 16:55:12.193904 15980 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 16:55:12.249995 15980 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start zero-shot learner...\n",
      "Done!\n",
      "Start transforming dataframe...\n",
      "Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = \"2012-01-01\"\n",
    "TRAIN_END_DATE = \"2015-12-31\"\n",
    "VALID_START_DATE = \"2016-01-01\"\n",
    "VALID_END_DATE = \"2016-12-31\"\n",
    "TEST_START_DATE = \"2017-01-01\"\n",
    "TEST_END_DATE = \"2020-07-01\"\n",
    "\n",
    "train = pd.DataFrame()\n",
    "valid = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "df_merge = joblib.load(\"../data/sp500_top100_v1.bin\")\n",
    "\n",
    "for ticker in tqdm(df_merge[\"ticker\"].unique()):\n",
    "    news = df_merge[df_merge[\"ticker\"] == str(ticker)]\n",
    "    news = load_news(news, labels=[\"finance\"], sort_by=\"finance\", k=3)\n",
    "    stock = load_stock(str(ticker), start_date=\"2012-01-01\")\n",
    "    news_and_stock = pd.merge(news, stock, on=[\"date\"])\n",
    "    news_and_stock.set_index('date', inplace=True)\n",
    "    \n",
    "    train_temp = news_and_stock.loc[pd.to_datetime(TRAIN_START_DATE).date():pd.to_datetime(TRAIN_END_DATE).date()]\n",
    "    valid_temp = news_and_stock.loc[pd.to_datetime(VALID_START_DATE).date():pd.to_datetime(VALID_END_DATE).date()]\n",
    "    test_temp = news_and_stock.loc[pd.to_datetime(TEST_START_DATE).date():pd.to_datetime(TEST_END_DATE).date()]\n",
    "    \n",
    "    train = pd.concat([train, train_temp], axis=0)\n",
    "    valid = pd.concat([valid, valid_temp], axis=0)\n",
    "    test = pd.concat([test, test_temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manager = multiprocessing.Manager()\n",
    "# return_dict = manager.dict()\n",
    "# jobs = []\n",
    "# for ticker in df_merge[\"ticker\"].unique():\n",
    "#     p = multiprocessing.Process(target=merge_function, args=(df_merge, ticker, return_dict))\n",
    "#     jobs.append(p)\n",
    "#     p.start()\n",
    "# for proc in jobs:\n",
    "#     proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/test.bin']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train, \"../data/train.bin\", compress=5)\n",
    "joblib.dump(valid, \"../data/valid.bin\", compress=5)\n",
    "joblib.dump(test, \"../data/test.bin\", compress=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 1 News</th>\n",
       "      <th>Top 2 News</th>\n",
       "      <th>Top 3 News</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>apple hires adobe officer to lead iad: report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.71</td>\n",
       "      <td>51.28</td>\n",
       "      <td>50.62</td>\n",
       "      <td>51.13</td>\n",
       "      <td>65005500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>refile-taiwan is elan says apple to pay $5 mln...</td>\n",
       "      <td>taiwan is elan says apple to pay $5 million in...</td>\n",
       "      <td>update 1-apple to pay elan $5 mln to settle pa...</td>\n",
       "      <td>51.32</td>\n",
       "      <td>51.76</td>\n",
       "      <td>51.04</td>\n",
       "      <td>51.70</td>\n",
       "      <td>67817400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-09</th>\n",
       "      <td>apple is cook got rich stock award worth $376 ...</td>\n",
       "      <td>apple is siri puts voice-enabled search in spo...</td>\n",
       "      <td>chinese authors sue apple for copyright infrin...</td>\n",
       "      <td>52.62</td>\n",
       "      <td>52.90</td>\n",
       "      <td>52.11</td>\n",
       "      <td>52.16</td>\n",
       "      <td>98506100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-10</th>\n",
       "      <td>kodak sues apple, htc over digital image patents</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.67</td>\n",
       "      <td>52.68</td>\n",
       "      <td>52.13</td>\n",
       "      <td>52.34</td>\n",
       "      <td>64549100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-11</th>\n",
       "      <td>strong apple contingent expected at ces</td>\n",
       "      <td>corrected-ces-strong apple contingent expected...</td>\n",
       "      <td>apple plans january 19 education event in new ...</td>\n",
       "      <td>52.27</td>\n",
       "      <td>52.29</td>\n",
       "      <td>51.86</td>\n",
       "      <td>52.26</td>\n",
       "      <td>53771200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-24</th>\n",
       "      <td>fitch upgrades fidelity national information s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49.69</td>\n",
       "      <td>50.60</td>\n",
       "      <td>49.69</td>\n",
       "      <td>50.21</td>\n",
       "      <td>1004200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-29</th>\n",
       "      <td>servicenow is revenue jumps 67 pct</td>\n",
       "      <td>servicenow is revenue jumps 67 percent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.19</td>\n",
       "      <td>58.22</td>\n",
       "      <td>56.44</td>\n",
       "      <td>57.60</td>\n",
       "      <td>1998900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-04-24</th>\n",
       "      <td>corrected-servicenow 1st-quarter revenue beats...</td>\n",
       "      <td>servicenow sees second-quarter, 2014 revenue a...</td>\n",
       "      <td>update 1-servicenow sees q2, 2014 revenue abov...</td>\n",
       "      <td>54.70</td>\n",
       "      <td>54.70</td>\n",
       "      <td>45.07</td>\n",
       "      <td>49.88</td>\n",
       "      <td>10440700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-10</th>\n",
       "      <td>four hewlett-packard patents invalidated in ca...</td>\n",
       "      <td>four hewlett-packard patents invalidated in se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.60</td>\n",
       "      <td>74.07</td>\n",
       "      <td>72.61</td>\n",
       "      <td>73.59</td>\n",
       "      <td>873500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-11</th>\n",
       "      <td>update 2-four hewlett-packard patents invalida...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.75</td>\n",
       "      <td>74.81</td>\n",
       "      <td>73.44</td>\n",
       "      <td>74.24</td>\n",
       "      <td>710200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20232 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Top 1 News  \\\n",
       "date                                                            \n",
       "2012-01-04      apple hires adobe officer to lead iad: report   \n",
       "2012-01-05  refile-taiwan is elan says apple to pay $5 mln...   \n",
       "2012-01-09  apple is cook got rich stock award worth $376 ...   \n",
       "2012-01-10   kodak sues apple, htc over digital image patents   \n",
       "2012-01-11            strong apple contingent expected at ces   \n",
       "...                                                       ...   \n",
       "2014-02-24  fitch upgrades fidelity national information s...   \n",
       "2014-01-29                 servicenow is revenue jumps 67 pct   \n",
       "2014-04-24  corrected-servicenow 1st-quarter revenue beats...   \n",
       "2015-03-10  four hewlett-packard patents invalidated in ca...   \n",
       "2015-03-11  update 2-four hewlett-packard patents invalida...   \n",
       "\n",
       "                                                   Top 2 News  \\\n",
       "date                                                            \n",
       "2012-01-04                                                NaN   \n",
       "2012-01-05  taiwan is elan says apple to pay $5 million in...   \n",
       "2012-01-09  apple is siri puts voice-enabled search in spo...   \n",
       "2012-01-10                                                NaN   \n",
       "2012-01-11  corrected-ces-strong apple contingent expected...   \n",
       "...                                                       ...   \n",
       "2014-02-24                                                NaN   \n",
       "2014-01-29             servicenow is revenue jumps 67 percent   \n",
       "2014-04-24  servicenow sees second-quarter, 2014 revenue a...   \n",
       "2015-03-10  four hewlett-packard patents invalidated in se...   \n",
       "2015-03-11                                                NaN   \n",
       "\n",
       "                                                   Top 3 News   Open   High  \\\n",
       "date                                                                          \n",
       "2012-01-04                                                NaN  50.71  51.28   \n",
       "2012-01-05  update 1-apple to pay elan $5 mln to settle pa...  51.32  51.76   \n",
       "2012-01-09  chinese authors sue apple for copyright infrin...  52.62  52.90   \n",
       "2012-01-10                                                NaN  52.67  52.68   \n",
       "2012-01-11  apple plans january 19 education event in new ...  52.27  52.29   \n",
       "...                                                       ...    ...    ...   \n",
       "2014-02-24                                                NaN  49.69  50.60   \n",
       "2014-01-29                                                NaN  58.19  58.22   \n",
       "2014-04-24  update 1-servicenow sees q2, 2014 revenue abov...  54.70  54.70   \n",
       "2015-03-10                                                NaN  73.60  74.07   \n",
       "2015-03-11                                                NaN  73.75  74.81   \n",
       "\n",
       "              Low  Close      Volume  Dividends  Stock Splits  label  \n",
       "date                                                                  \n",
       "2012-01-04  50.62  51.13  65005500.0        0.0           0.0      1  \n",
       "2012-01-05  51.04  51.70  67817400.0        0.0           0.0      1  \n",
       "2012-01-09  52.11  52.16  98506100.0        0.0           0.0      0  \n",
       "2012-01-10  52.13  52.34  64549100.0        0.0           0.0      1  \n",
       "2012-01-11  51.86  52.26  53771200.0        0.0           0.0      0  \n",
       "...           ...    ...         ...        ...           ...    ...  \n",
       "2014-02-24  49.69  50.21   1004200.0        0.0           0.0      1  \n",
       "2014-01-29  56.44  57.60   1998900.0        0.0           0.0      0  \n",
       "2014-04-24  45.07  49.88  10440700.0        0.0           0.0      0  \n",
       "2015-03-10  72.61  73.59    873500.0        0.0           0.0      0  \n",
       "2015-03-11  73.44  74.24    710200.0        0.0           0.0      1  \n",
       "\n",
       "[20232 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 1 News</th>\n",
       "      <th>Top 2 News</th>\n",
       "      <th>Top 3 News</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-04</th>\n",
       "      <td>apple shares off but weather u.s. market selloff</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>95.40</td>\n",
       "      <td>97.97</td>\n",
       "      <td>94.83</td>\n",
       "      <td>97.95</td>\n",
       "      <td>67649400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-05</th>\n",
       "      <td>apple expected to cut iphone 6s, 6s plus produ...</td>\n",
       "      <td>update 2-apple expected to cut iphone 6s, 6s p...</td>\n",
       "      <td>apple expected to cut iphone 6s, 6s plus produ...</td>\n",
       "      <td>98.32</td>\n",
       "      <td>98.41</td>\n",
       "      <td>95.22</td>\n",
       "      <td>95.49</td>\n",
       "      <td>55791000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-06</th>\n",
       "      <td>update 1-apple paid ceo tim cook $10.3 mln in ...</td>\n",
       "      <td>apple paid ceo tim cook $10.3 mln in 2015</td>\n",
       "      <td>apple paid ceo tim cook $10.3 million in 2015</td>\n",
       "      <td>93.49</td>\n",
       "      <td>95.18</td>\n",
       "      <td>92.85</td>\n",
       "      <td>93.63</td>\n",
       "      <td>68457400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-07</th>\n",
       "      <td>apple buys artificial intelligence startup emo...</td>\n",
       "      <td>update 1-apple suppliers cut revenue estimates...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.75</td>\n",
       "      <td>93.10</td>\n",
       "      <td>89.66</td>\n",
       "      <td>89.67</td>\n",
       "      <td>81094400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-08</th>\n",
       "      <td>apple registers automobile domain names, inclu...</td>\n",
       "      <td>update 1-major apple supplier hon hai of taiwa...</td>\n",
       "      <td>major apple supplier hon hai of taiwan posts 2...</td>\n",
       "      <td>91.63</td>\n",
       "      <td>92.15</td>\n",
       "      <td>89.96</td>\n",
       "      <td>90.15</td>\n",
       "      <td>70798000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-21</th>\n",
       "      <td>brief-servicenow q1 non-gaap earnings per shar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.77</td>\n",
       "      <td>76.84</td>\n",
       "      <td>73.21</td>\n",
       "      <td>74.27</td>\n",
       "      <td>8395500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-06-01</th>\n",
       "      <td>brief-servicenow acquires brightpoint security</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.38</td>\n",
       "      <td>73.50</td>\n",
       "      <td>71.06</td>\n",
       "      <td>73.17</td>\n",
       "      <td>1889700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-28</th>\n",
       "      <td>brief-servicenow q2 earnings per share view $0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.40</td>\n",
       "      <td>78.77</td>\n",
       "      <td>74.15</td>\n",
       "      <td>76.38</td>\n",
       "      <td>3991900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-27</th>\n",
       "      <td>brief-servicenow q3 gaap loss per share $0.22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.82</td>\n",
       "      <td>89.79</td>\n",
       "      <td>84.56</td>\n",
       "      <td>84.96</td>\n",
       "      <td>11112300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-09</th>\n",
       "      <td>brief-accenture acquires nashco consulting, ex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.99</td>\n",
       "      <td>86.05</td>\n",
       "      <td>83.34</td>\n",
       "      <td>84.98</td>\n",
       "      <td>1357300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5006 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Top 1 News  \\\n",
       "date                                                            \n",
       "2016-01-04   apple shares off but weather u.s. market selloff   \n",
       "2016-01-05  apple expected to cut iphone 6s, 6s plus produ...   \n",
       "2016-01-06  update 1-apple paid ceo tim cook $10.3 mln in ...   \n",
       "2016-01-07  apple buys artificial intelligence startup emo...   \n",
       "2016-01-08  apple registers automobile domain names, inclu...   \n",
       "...                                                       ...   \n",
       "2016-04-21  brief-servicenow q1 non-gaap earnings per shar...   \n",
       "2016-06-01     brief-servicenow acquires brightpoint security   \n",
       "2016-07-28  brief-servicenow q2 earnings per share view $0...   \n",
       "2016-10-27      brief-servicenow q3 gaap loss per share $0.22   \n",
       "2016-11-09  brief-accenture acquires nashco consulting, ex...   \n",
       "\n",
       "                                                   Top 2 News  \\\n",
       "date                                                            \n",
       "2016-01-04                                                NaN   \n",
       "2016-01-05  update 2-apple expected to cut iphone 6s, 6s p...   \n",
       "2016-01-06          apple paid ceo tim cook $10.3 mln in 2015   \n",
       "2016-01-07  update 1-apple suppliers cut revenue estimates...   \n",
       "2016-01-08  update 1-major apple supplier hon hai of taiwa...   \n",
       "...                                                       ...   \n",
       "2016-04-21                                                NaN   \n",
       "2016-06-01                                                NaN   \n",
       "2016-07-28                                                NaN   \n",
       "2016-10-27                                                NaN   \n",
       "2016-11-09                                                NaN   \n",
       "\n",
       "                                                   Top 3 News   Open   High  \\\n",
       "date                                                                          \n",
       "2016-01-04                                                NaN  95.40  97.97   \n",
       "2016-01-05  apple expected to cut iphone 6s, 6s plus produ...  98.32  98.41   \n",
       "2016-01-06      apple paid ceo tim cook $10.3 million in 2015  93.49  95.18   \n",
       "2016-01-07                                                NaN  91.75  93.10   \n",
       "2016-01-08  major apple supplier hon hai of taiwan posts 2...  91.63  92.15   \n",
       "...                                                       ...    ...    ...   \n",
       "2016-04-21                                                NaN  74.77  76.84   \n",
       "2016-06-01                                                NaN  71.38  73.50   \n",
       "2016-07-28                                                NaN  75.40  78.77   \n",
       "2016-10-27                                                NaN  86.82  89.79   \n",
       "2016-11-09                                                NaN  83.99  86.05   \n",
       "\n",
       "              Low  Close      Volume  Dividends  Stock Splits  label  \n",
       "date                                                                  \n",
       "2016-01-04  94.83  97.95  67649400.0        0.0           0.0      1  \n",
       "2016-01-05  95.22  95.49  55791000.0        0.0           0.0      0  \n",
       "2016-01-06  92.85  93.63  68457400.0        0.0           0.0      0  \n",
       "2016-01-07  89.66  89.67  81094400.0        0.0           0.0      0  \n",
       "2016-01-08  89.96  90.15  70798000.0        0.0           0.0      1  \n",
       "...           ...    ...         ...        ...           ...    ...  \n",
       "2016-04-21  73.21  74.27   8395500.0        0.0           0.0      1  \n",
       "2016-06-01  71.06  73.17   1889700.0        0.0           0.0      1  \n",
       "2016-07-28  74.15  76.38   3991900.0        0.0           0.0      1  \n",
       "2016-10-27  84.56  84.96  11112300.0        0.0           0.0      1  \n",
       "2016-11-09  83.34  84.98   1357300.0        0.0           0.0      0  \n",
       "\n",
       "[5006 rows x 11 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Top 1 News</th>\n",
       "      <th>Top 2 News</th>\n",
       "      <th>Top 3 News</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <td>apple confirms $1 bln investment in softbank t...</td>\n",
       "      <td>update 2-apple confirms $1 bln investment in s...</td>\n",
       "      <td>apple confirms $1 billion investment in softba...</td>\n",
       "      <td>110.11</td>\n",
       "      <td>110.73</td>\n",
       "      <td>110.01</td>\n",
       "      <td>110.27</td>\n",
       "      <td>21118100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-05</th>\n",
       "      <td>brief-apple says app store generated over $20 ...</td>\n",
       "      <td>update 1-apple is app store generated $20 bln ...</td>\n",
       "      <td>apple is app store generated $20 billion for d...</td>\n",
       "      <td>110.17</td>\n",
       "      <td>111.07</td>\n",
       "      <td>110.07</td>\n",
       "      <td>110.83</td>\n",
       "      <td>22193600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-06</th>\n",
       "      <td>brief-apple inc is ceo tim cook is total 2016 ...</td>\n",
       "      <td>canada is competition watchdog closes two-year...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110.99</td>\n",
       "      <td>112.30</td>\n",
       "      <td>110.70</td>\n",
       "      <td>112.07</td>\n",
       "      <td>31751900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-10</th>\n",
       "      <td>china is wechat seeks slice of apple is app st...</td>\n",
       "      <td>china is wechat seeks slice of apple is app st...</td>\n",
       "      <td>tesla taps apple engineer for autopilot software</td>\n",
       "      <td>112.88</td>\n",
       "      <td>113.46</td>\n",
       "      <td>112.44</td>\n",
       "      <td>113.21</td>\n",
       "      <td>24462100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-12</th>\n",
       "      <td>u.s. appeals court revives antitrust lawsuit a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.01</td>\n",
       "      <td>113.39</td>\n",
       "      <td>112.35</td>\n",
       "      <td>113.34</td>\n",
       "      <td>27086200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>brief-servicenow releases four emergency respo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.41</td>\n",
       "      <td>302.98</td>\n",
       "      <td>270.02</td>\n",
       "      <td>287.42</td>\n",
       "      <td>3908100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-21</th>\n",
       "      <td>brief-lincoln financial announces steps to sup...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298.96</td>\n",
       "      <td>303.70</td>\n",
       "      <td>281.57</td>\n",
       "      <td>288.77</td>\n",
       "      <td>2892800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-30</th>\n",
       "      <td>u.s. research roundup- labcorp, servicenow, ze...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.84</td>\n",
       "      <td>357.78</td>\n",
       "      <td>327.49</td>\n",
       "      <td>351.54</td>\n",
       "      <td>5387700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-05</th>\n",
       "      <td>servicenow, adobe pair their customer service ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.00</td>\n",
       "      <td>376.18</td>\n",
       "      <td>356.07</td>\n",
       "      <td>370.64</td>\n",
       "      <td>2977200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-26</th>\n",
       "      <td>nike plans to cut jobs in digital push</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.83</td>\n",
       "      <td>402.84</td>\n",
       "      <td>391.08</td>\n",
       "      <td>399.97</td>\n",
       "      <td>2013100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18942 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Top 1 News  \\\n",
       "date                                                            \n",
       "2017-01-04  apple confirms $1 bln investment in softbank t...   \n",
       "2017-01-05  brief-apple says app store generated over $20 ...   \n",
       "2017-01-06  brief-apple inc is ceo tim cook is total 2016 ...   \n",
       "2017-01-10  china is wechat seeks slice of apple is app st...   \n",
       "2017-01-12  u.s. appeals court revives antitrust lawsuit a...   \n",
       "...                                                       ...   \n",
       "2020-03-17  brief-servicenow releases four emergency respo...   \n",
       "2020-04-21  brief-lincoln financial announces steps to sup...   \n",
       "2020-04-30  u.s. research roundup- labcorp, servicenow, ze...   \n",
       "2020-05-05  servicenow, adobe pair their customer service ...   \n",
       "2020-06-26             nike plans to cut jobs in digital push   \n",
       "\n",
       "                                                   Top 2 News  \\\n",
       "date                                                            \n",
       "2017-01-04  update 2-apple confirms $1 bln investment in s...   \n",
       "2017-01-05  update 1-apple is app store generated $20 bln ...   \n",
       "2017-01-06  canada is competition watchdog closes two-year...   \n",
       "2017-01-10  china is wechat seeks slice of apple is app st...   \n",
       "2017-01-12                                                NaN   \n",
       "...                                                       ...   \n",
       "2020-03-17                                                NaN   \n",
       "2020-04-21                                                NaN   \n",
       "2020-04-30                                                NaN   \n",
       "2020-05-05                                                NaN   \n",
       "2020-06-26                                                NaN   \n",
       "\n",
       "                                                   Top 3 News    Open    High  \\\n",
       "date                                                                            \n",
       "2017-01-04  apple confirms $1 billion investment in softba...  110.11  110.73   \n",
       "2017-01-05  apple is app store generated $20 billion for d...  110.17  111.07   \n",
       "2017-01-06                                                NaN  110.99  112.30   \n",
       "2017-01-10   tesla taps apple engineer for autopilot software  112.88  113.46   \n",
       "2017-01-12                                                NaN  113.01  113.39   \n",
       "...                                                       ...     ...     ...   \n",
       "2020-03-17                                                NaN  271.41  302.98   \n",
       "2020-04-21                                                NaN  298.96  303.70   \n",
       "2020-04-30                                                NaN  327.84  357.78   \n",
       "2020-05-05                                                NaN  360.00  376.18   \n",
       "2020-06-26                                                NaN  401.83  402.84   \n",
       "\n",
       "               Low   Close      Volume  Dividends  Stock Splits  label  \n",
       "date                                                                    \n",
       "2017-01-04  110.01  110.27  21118100.0        0.0           0.0      0  \n",
       "2017-01-05  110.07  110.83  22193600.0        0.0           0.0      1  \n",
       "2017-01-06  110.70  112.07  31751900.0        0.0           0.0      1  \n",
       "2017-01-10  112.44  113.21  24462100.0        0.0           0.0      1  \n",
       "2017-01-12  112.35  113.34  27086200.0        0.0           0.0      0  \n",
       "...            ...     ...         ...        ...           ...    ...  \n",
       "2020-03-17  270.02  287.42   3908100.0        0.0           0.0      1  \n",
       "2020-04-21  281.57  288.77   2892800.0        0.0           0.0      0  \n",
       "2020-04-30  327.49  351.54   5387700.0        0.0           0.0      1  \n",
       "2020-05-05  356.07  370.64   2977200.0        0.0           0.0      1  \n",
       "2020-06-26  391.08  399.97   2013100.0        0.0           0.0      0  \n",
       "\n",
       "[18942 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
