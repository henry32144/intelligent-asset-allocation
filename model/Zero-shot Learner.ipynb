{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 14:37:59.328766  4416 file_utils.py:39] PyTorch version 1.2.0 available.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import string\n",
    "import joblib\n",
    "import multiprocessing\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from torch.nn import functional as F\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBert():\n",
    "    \"\"\"\n",
    "    A common approach to zero shot learning using Sentence-BERT.\n",
    "    Reference from https://joeddav.github.io/blog/2020/05/29/ZSL.html\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "        self.model = AutoModel.from_pretrained('deepset/sentence_bert')\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "    def get_similarity(self, sentence, labels):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            sentence: str\n",
    "            label: list\n",
    "        \"\"\"\n",
    "        # Run inputs through model and mean-pool over the sequence dimension to get sequence-level representations\n",
    "        inputs = self.tokenizer.batch_encode_plus(\n",
    "            [sentence] + labels,\n",
    "            return_tensors='pt',\n",
    "            pad_to_max_length=True)\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        attention_mask = inputs['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_ids, attention_mask=attention_mask)[0]\n",
    "        sentence_rep = output[:1].mean(dim=1)\n",
    "        label_reps = output[1:].mean(dim=1)\n",
    "    \n",
    "        # Now find the labels with the highest cosine similarities to the sentence\n",
    "        similarities = F.cosine_similarity(sentence_rep, label_reps)\n",
    "        closest = similarities.argsort(descending=True)\n",
    "        \n",
    "        sim_dict = defaultdict()\n",
    "        for ind in closest:\n",
    "            sim_dict[labels[ind]] = (similarities[ind].item())\n",
    "            \n",
    "        return sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = joblib.load(\"reuters_news.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 14:38:02.867355  4416 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0708 14:38:02.871345  4416 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 14:38:02.873347  4416 tokenization_utils.py:929] Model name 'deepset/sentence_bert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'deepset/sentence_bert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0708 14:38:06.402196  4416 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/vocab.txt from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\205379d98ab8dc0f29c84c5c1c03e3bfef4cd7d58a9d0f6f18636389f3339834.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0708 14:38:06.404192  4416 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/added_tokens.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\989171ad3bb37d75ff0320403ee6750dfd91417f67b1c2e8d2baa87b4898ca9c.3889713104075cfee9e96090bcdd0dc753733b3db9da20d1dd8b2cd1030536a2\n",
      "I0708 14:38:06.406193  4416 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/special_tokens_map.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\07735ccda4d1cf040b9ee6c711c29b65caa6632a10c406c15d0849fcbfbce9a0.275045728fbf41c11d3dae08b8742c054377e18d92cc7b72b6351152a99b64e4\n",
      "I0708 14:38:06.409151  4416 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/tokenizer_config.json from cache at None\n",
      "I0708 14:38:07.325729  4416 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/deepset/sentence_bert/config.json from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\1a89cc2d2dfc0e9beb7d49442e942849b45bbbf49d0b004f6be414b44c4e01fa.f9ed8a8332fc340fb779f9e83f1745369bbe51a3ac3e1c0d0dc5c3cf72ef4626\n",
      "I0708 14:38:07.328692  4416 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0708 14:38:07.781472  4416 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/deepset/sentence_bert/pytorch_model.bin from cache at C:\\Users\\YangWang/.cache\\torch\\transformers\\fa9d12cb00cd5a31f5a5367f58d242199473a6deb02c51380681ade7bf33c713.4948a08b5d844db1ecda79f6e7f47643f0175f2c030d48ce8b3beee3c6bd6012\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dc6a3fe38b4e2fac9703c639b83d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labels = ['forex', 'finance', 'stocks']\n",
    "SB = SentenceBert()\n",
    "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    sim_dict = SB.get_similarity(row[\"title\"], labels)\n",
    "    for i in range(len(labels)):   \n",
    "        df.loc[index, labels[i]] = sim_dict[labels[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>forex</th>\n",
       "      <th>finance</th>\n",
       "      <th>stocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beware of debt costs and an inflationary bite,...</td>\n",
       "      <td>2020-07-05 19:00:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>0.498809</td>\n",
       "      <td>0.287072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beware of debt costs and an inflationary bite,...</td>\n",
       "      <td>2020-07-05 19:17:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>0.498809</td>\n",
       "      <td>0.287072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deals of the day-Mergers and acquisitions</td>\n",
       "      <td>2020-07-07 16:00:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.157465</td>\n",
       "      <td>0.358413</td>\n",
       "      <td>0.328640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zoom rolls out hardware subscription service</td>\n",
       "      <td>2020-07-07 10:48:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.101178</td>\n",
       "      <td>0.283011</td>\n",
       "      <td>0.171321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zoom rolls out hardware subscription service</td>\n",
       "      <td>2020-07-07 09:58:00-08:00</td>\n",
       "      <td>google</td>\n",
       "      <td>0.101178</td>\n",
       "      <td>0.283011</td>\n",
       "      <td>0.171321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Beware of debt costs and an inflationary bite,...   \n",
       "1  Beware of debt costs and an inflationary bite,...   \n",
       "2          Deals of the day-Mergers and acquisitions   \n",
       "3       Zoom rolls out hardware subscription service   \n",
       "4       Zoom rolls out hardware subscription service   \n",
       "\n",
       "                       date   query     forex   finance    stocks  \n",
       "0 2020-07-05 19:00:00-08:00  google  0.064136  0.498809  0.287072  \n",
       "1 2020-07-05 19:17:00-08:00  google  0.064136  0.498809  0.287072  \n",
       "2 2020-07-07 16:00:00-08:00  google  0.157465  0.358413  0.328640  \n",
       "3 2020-07-07 10:48:00-08:00  google  0.101178  0.283011  0.171321  \n",
       "4 2020-07-07 09:58:00-08:00  google  0.101178  0.283011  0.171321  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by=\"finance\", axis=0, ascending=False)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
